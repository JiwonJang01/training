{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b3b590-7a62-45fc-9908-74d85f3a67cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[11] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe -> RDD\n",
    "spark.range(10).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d1b156-5e03-4e7f-a88c-7ba04062e080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).toDF(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b22bb00f-ebfc-4936-b218-995583d3491b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[22] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).toDF(\"id\").rdd.map(lambda row: roe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95720850-98b3-4e83-9262-0919a3a08af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD 호출 후 toDF는 일반적이지 않음\n",
    "spark.range(10).rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623e940f-9a20-4175-9166-745462d4a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "myCollection = \"Spark The Definitive Guide: Big Data Processing Made Simple\".split()\n",
    "words = spark.sparkContext.parallelize(myCollection,2)  # 2 partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9afe3d7-4003-4fd5-bc6f-fe24b67824c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장의 길이를 구한다.\n",
    "# w_length = words.map(lambda w : len(w))\n",
    "# w_length.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd240024-4921-4844-bf79-b2b6522696a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'myWords'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.setName(\"myWords\")\n",
    "words.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b03c6093-b2ef-492f-b27e-7df85aa95360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Spark', 'Simple']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD filtering\n",
    "def startsWithS(data):\n",
    "    return data.startswith(\"S\")\n",
    "\n",
    "words.filter(lambda w: startsWithS(w)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6aec596-73be-4be9-a2ce-7fcebb689293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[67] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2 = words.map(lambda word: (word,word[0],word.startswith(\"S\")))\n",
    "words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc1602cf-3fa2-4c51-86f8-c874b2fbd8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 'S', True), ('Simple', 'S', True)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 5개의 레코드를 찾아서 반환\n",
    "words2.filter(lambda record : record[2]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46f0854e-54e1-4acb-b9dd-db824189f74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'p', 'a', 'r', 'k', 'T', 'h', 'e', 'D', 'e']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트로 반환\n",
    "words.flatMap(lambda word:list(word)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9080192-f16f-4c70-bafb-ac471f54130e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'p', 'a', 'r', 'k', ' ', 'T', 'h', 'e']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"Spark The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc8c9104-dd85-47bc-bd1b-49339cfe2a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Definitive', 'Processing']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sortBy(lambda word: len(word)*-1).take(2)  \n",
    "# 단어의 길이를 계산해서 음수로 변경...정렬의 기본은 오름차순-->내림차순\n",
    "# 단어의 길이가 가장 긴 것 부터 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed39ce85-bf67-4c08-8c7d-2452174d738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_50 = words.randomSplit([0.5,0.5]) # RDD 50% 두 부분으로 나눈다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f959be93-6235-4360-b8e8-6065fd02e729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce의 기능 1~20까지의 total summary  range(1,21)\n",
    "spark.sparkContext.parallelize(range(1,21)).reduce(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93b4b86d-3157-4ac5-a476-11b0975ac6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 개의 단어 중에서 길이가 긴 단어를 선택 reducer\n",
    "def wordLengthReducer(leftWord, rightWord):\n",
    "    return leftWord if len(leftWord) > len(rigthWord) else rightWord\n",
    "\n",
    "    # if leftWord> rightWord:\n",
    "    #     return leftWord\n",
    "    # else:\n",
    "    #     return rigthWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a54d9396-1dca-40e7-9069-8e45f50571c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/25 10:37:59 ERROR Executor: Exception in task 0.0 in stage 31.0 (TID 50)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/root/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/root/spark/python/pyspark/rdd.py\", line 1922, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/root/spark/python/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3282/146314216.py\", line 3, in wordLengthReducer\n",
      "NameError: name 'rigthWord' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/03/25 10:37:59 WARN TaskSetManager: Lost task 0.0 in stage 31.0 (TID 50) (192.168.51.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/root/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/root/spark/python/pyspark/rdd.py\", line 1922, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/root/spark/python/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3282/146314216.py\", line 3, in wordLengthReducer\n",
      "NameError: name 'rigthWord' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "24/03/25 10:37:59 ERROR TaskSetManager: Task 0 in stage 31.0 failed 1 times; aborting job\n",
      "24/03/25 10:38:00 WARN TaskSetManager: Lost task 1.0 in stage 31.0 (TID 51) (192.168.51.128 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 50) (192.168.51.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/root/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/root/spark/python/pyspark/rdd.py\", line 1922, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/root/spark/python/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3282/146314216.py\", line 3, in wordLengthReducer\n",
      "NameError: name 'rigthWord' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 50) (192.168.51.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/root/spark/python/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n  File \"/root/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_3282/146314216.py\", line 3, in wordLengthReducer\nNameError: name 'rigthWord' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/root/spark/python/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n  File \"/root/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_3282/146314216.py\", line 3, in wordLengthReducer\nNameError: name 'rigthWord' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwordLengthReducer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m~/spark/python/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 50) (192.168.51.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/root/spark/python/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n  File \"/root/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_3282/146314216.py\", line 3, in wordLengthReducer\nNameError: name 'rigthWord' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/root/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/root/spark/python/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n  File \"/root/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_3282/146314216.py\", line 3, in wordLengthReducer\nNameError: name 'rigthWord' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dc6ac8f-1739-4bb8-b650-603dd246f60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Processing'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 개의 단어 중에서 길이가 긴 단어를 선택 reducer\n",
    "def wordLengthReducer(leftWord, rightWord):\n",
    "    return leftWord if len(leftWord) > len(rightWord) else rightWord\n",
    "\n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9920be7b-6306-47f1-ad25-7555c6d99b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파티션 개수 확인하기\n",
    "words.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd529ea0-e818-4f4e-ad4a-cedf54d73685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.mapPartitions(lambda part : [1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad34f3de-f198-438b-a1ae-e1c5cb4e3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 파티션의 인덱스와 해당 파티션에 속한 각 요소를 사용 새로운 값을 생성\n",
    "def indexedFunc(partitionIndex, withinPartIterator):\n",
    "    return [f\"partition:{partitionIndex} => {x}\" for x in withinPartIterator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "048665a9-4355-4e97-8da2-fbe0d972de60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partition:0 => Spark',\n",
       " 'partition:0 => The',\n",
       " 'partition:0 => Definitive',\n",
       " 'partition:0 => Guide:',\n",
       " 'partition:1 => Big',\n",
       " 'partition:1 => Data',\n",
       " 'partition:1 => Processing',\n",
       " 'partition:1 => Made',\n",
       " 'partition:1 => Simple']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.mapPartitionsWithIndex(indexedFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a04d2816-d02d-4871-9f72-32ba9192f98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello'], ['Word']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gloam() : RDD의 요소ㄹ 파티션 별로 그룹화->리스트로 반환 RDD의 구조를 확인하거나 디버깅 용도\n",
    "spark.sparkContext.parallelize([\"Hello\",\"Word\"],2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc066e56-d64d-4a03-96ef-b88d504ff639",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = '''\n",
    " Prospects of talks between the government and the medical community over a prolonged walkout by junior doctors were raised as Yoon instructed officials to seek dialogue with doctors, with the walkout disrupting medical services at major hospitals for nearly five weeks.\n",
    "\"Related ministries have immediately launched working-level preparations to hold talks with the medical community,\" Cho told a government response meeting. \"We will arrange a meeting between the government and the medical community as soon as possible.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f630a70-8b19-4adc-8567-59d68b2f0497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[106] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = spark.sparkContext.parallelize(words)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8203df52-1452-49e0-b29c-165dbc2c3158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7a1c760-3831-4ffd-9d36-fdb0f945acc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P', 'r', 'o', 's', 'p']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.flatMap(lambda line:line.split()).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a360978b-3c94-4f5b-b026-9cdfe16215d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = text.flatMap(lambda line:line.split()).map(lambda word:(word,1)).reduceByKey(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cfa12689-8317-4ea9-aab5-f82dc6e7ae27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P', 1),\n",
       " ('r', 24),\n",
       " ('o', 36),\n",
       " ('s', 27),\n",
       " ('p', 8),\n",
       " ('e', 57),\n",
       " ('c', 14),\n",
       " ('t', 38),\n",
       " ('f', 5),\n",
       " ('a', 31),\n",
       " ('l', 23),\n",
       " ('k', 7),\n",
       " ('b', 4),\n",
       " ('w', 11),\n",
       " ('n', 28),\n",
       " ('h', 14),\n",
       " ('g', 10),\n",
       " ('v', 8),\n",
       " ('m', 19),\n",
       " ('d', 18),\n",
       " ('i', 32),\n",
       " ('u', 10),\n",
       " ('y', 6),\n",
       " ('j', 2),\n",
       " ('Y', 1),\n",
       " (',', 2),\n",
       " ('.', 3),\n",
       " ('\"', 4),\n",
       " ('R', 1),\n",
       " ('-', 1),\n",
       " ('C', 1),\n",
       " ('W', 1)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb74ed89-4116-44f8-a29e-02ba78d9334f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each multiple2\n",
    "numbers = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "result = numbers.map(lambda x:x*2)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e7d7ea54-72a5-4f4d-9724-741e7403dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n",
      "[3, 3, 9]\n",
      "['aaa', 'abc', 'bbbbbbbbb']\n"
     ]
    }
   ],
   "source": [
    "# 짝수만 필터링 filter\n",
    "print(numbers.filter(lambda x:x%2==0).collect())\n",
    "\n",
    "# 모든 요소의 합 reduce\n",
    "temp = spark.sparkContext.parallelize([\"abc\",\"aaa\",\"bbbbbbbbb\"])\n",
    "\n",
    "# 각 요소의 길이도 map\n",
    "print( temp.map(lambda x: len(x)).collect() )\n",
    "\n",
    "# 정렬 sortBy\n",
    "print(temp.sortBy(lambda x : x).collect() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c9ca839-fc19-473c-acc5-c16f2498c168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 10, 20, 10, 100, 200]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD 2개를 분할해서 합치기\n",
    "rdd1 = spark.sparkContext.parallelize([1,10,20])\n",
    "rdd2 = spark.sparkContext.parallelize([10,100,200])\n",
    "merged_rdd = rdd1.union(rdd2)\n",
    "merged_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14b03e20-7427-4822-a97a-81d2537f1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv파일 RDD로 열기\n",
    "read_csv_ = \"bydata/by-day/2011-12-09.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e26e12d1-2f06-4b17-9b24-2921b7d1fa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_lines = spark.sparkContext.textFile(read_csv_)\n",
    "header = csv_lines.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1b602745-9a89-4ddd-8cbf-54467d6d1874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b5eda661-07bd-4d89-af3b-fa43a6bd586b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['581475,22596,CHRISTMAS STAR WISH LIST CHALKBOARD,36,2011-12-09 08:39:00,0.39,13069.0,United Kingdom',\n",
       " '581475,23235,STORAGE TIN VINTAGE LEAF,12,2011-12-09 08:39:00,1.25,13069.0,United Kingdom',\n",
       " '581475,23272,TREE T-LIGHT HOLDER WILLIE WINKIE,12,2011-12-09 08:39:00,0.39,13069.0,United Kingdom',\n",
       " '581475,23239,SET OF 4 KNICK KNACK TINS POPPIES,6,2011-12-09 08:39:00,1.65,13069.0,United Kingdom',\n",
       " '581475,21705,BAG 500g SWIRLY MARBLES,24,2011-12-09 08:39:00,0.39,13069.0,United Kingdom']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove header\n",
    "data = csv_lines.filter(lambda line : line != header)\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cb51e3fa-098e-4132-9ac3-1afcf7abb0cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['InvoiceNo',\n",
       "  'StockCode',\n",
       "  'Description',\n",
       "  'Quantity',\n",
       "  'InvoiceDate',\n",
       "  'UnitPrice',\n",
       "  'CustomerID',\n",
       "  'Country'],\n",
       " ['581475',\n",
       "  '22596',\n",
       "  'CHRISTMAS STAR WISH LIST CHALKBOARD',\n",
       "  '36',\n",
       "  '2011-12-09 08:39:00',\n",
       "  '0.39',\n",
       "  '13069.0',\n",
       "  'United Kingdom'],\n",
       " ['581475',\n",
       "  '23235',\n",
       "  'STORAGE TIN VINTAGE LEAF',\n",
       "  '12',\n",
       "  '2011-12-09 08:39:00',\n",
       "  '1.25',\n",
       "  '13069.0',\n",
       "  'United Kingdom'],\n",
       " ['581475',\n",
       "  '23272',\n",
       "  'TREE T-LIGHT HOLDER WILLIE WINKIE',\n",
       "  '12',\n",
       "  '2011-12-09 08:39:00',\n",
       "  '0.39',\n",
       "  '13069.0',\n",
       "  'United Kingdom'],\n",
       " ['581475',\n",
       "  '23239',\n",
       "  'SET OF 4 KNICK KNACK TINS POPPIES',\n",
       "  '6',\n",
       "  '2011-12-09 08:39:00',\n",
       "  '1.65',\n",
       "  '13069.0',\n",
       "  'United Kingdom']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_lines.map(lambda x: x.split(\",\")).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d22f242-6ce1-41b3-bc3b-9b732dbd7102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('InvoiceNo', 'StockCode', 'Description'),\n",
       " ('581475', '22596', 'CHRISTMAS STAR WISH LIST CHALKBOARD'),\n",
       " ('581475', '23235', 'STORAGE TIN VINTAGE LEAF'),\n",
       " ('581475', '23272', 'TREE T-LIGHT HOLDER WILLIE WINKIE'),\n",
       " ('581475', '23239', 'SET OF 4 KNICK KNACK TINS POPPIES')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda x : x.split(\",\")).map(lambda x: (x[0],x[1],x[2]) ).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f2df22c9-550e-44cd-aaaa-32aa44f49e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country',\n",
       " '581475,22596,CHRISTMAS STAR WISH LIST CHALKBOARD,36,2011-12-09 08:39:00,0.39,13069.0,United Kingdom']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 읽을 때 빈 줄을 제거하고 읽기\n",
    "csv_lines = spark.sparkContext.textFile(read_csv_)\n",
    "non_empty_lines = csv_lines.filter(lambda line : line.split() != \"\")\n",
    "non_empty_lines.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "245100ff-7b2f-4243-9dab-051e2bc17374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country',\n",
       " '581475,22596,CHRISTMAS STAR WISH LIST CHALKBOARD,36,2011-12-09 08:39:00,0.39,13069.0,United Kingdom']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#중복된 행 제거\n",
    "data = non_empty_lines.distinct()\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "46d15c12-3d40-45e9-b700-84bc2253e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 폴더하위에 있는 모든 csv파일 읽어서 RDD로 합치기 glob or os\n",
    "# spark.sparkContext.union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "41c73b2c-b84e-4e3e-aaa5-2a5cacf7fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b8f3a9dd-eda6-481f-b991-a44c11be6108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepaths = \"bydata/by-day/*.csv\"\n",
    "files = glob(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a5605fa1-f173-4898-b1e7-f3c1928ccc85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bydata/by-day/2010-12-01.csv',\n",
       " 'bydata/by-day/2010-12-02.csv',\n",
       " 'bydata/by-day/2010-12-03.csv',\n",
       " 'bydata/by-day/2010-12-05.csv',\n",
       " 'bydata/by-day/2010-12-06.csv',\n",
       " 'bydata/by-day/2010-12-07.csv',\n",
       " 'bydata/by-day/2010-12-08.csv',\n",
       " 'bydata/by-day/2010-12-09.csv',\n",
       " 'bydata/by-day/2010-12-10.csv',\n",
       " 'bydata/by-day/2010-12-12.csv',\n",
       " 'bydata/by-day/2010-12-13.csv',\n",
       " 'bydata/by-day/2010-12-14.csv',\n",
       " 'bydata/by-day/2010-12-15.csv',\n",
       " 'bydata/by-day/2010-12-16.csv',\n",
       " 'bydata/by-day/2010-12-17.csv',\n",
       " 'bydata/by-day/2010-12-19.csv',\n",
       " 'bydata/by-day/2010-12-20.csv',\n",
       " 'bydata/by-day/2010-12-21.csv',\n",
       " 'bydata/by-day/2010-12-22.csv',\n",
       " 'bydata/by-day/2010-12-23.csv',\n",
       " 'bydata/by-day/2011-01-04.csv',\n",
       " 'bydata/by-day/2011-01-05.csv',\n",
       " 'bydata/by-day/2011-01-06.csv',\n",
       " 'bydata/by-day/2011-01-07.csv',\n",
       " 'bydata/by-day/2011-01-09.csv',\n",
       " 'bydata/by-day/2011-01-10.csv',\n",
       " 'bydata/by-day/2011-01-11.csv',\n",
       " 'bydata/by-day/2011-01-12.csv',\n",
       " 'bydata/by-day/2011-01-13.csv',\n",
       " 'bydata/by-day/2011-01-14.csv',\n",
       " 'bydata/by-day/2011-01-16.csv',\n",
       " 'bydata/by-day/2011-01-17.csv',\n",
       " 'bydata/by-day/2011-01-18.csv',\n",
       " 'bydata/by-day/2011-01-19.csv',\n",
       " 'bydata/by-day/2011-01-20.csv',\n",
       " 'bydata/by-day/2011-01-21.csv',\n",
       " 'bydata/by-day/2011-01-23.csv',\n",
       " 'bydata/by-day/2011-01-24.csv',\n",
       " 'bydata/by-day/2011-01-25.csv',\n",
       " 'bydata/by-day/2011-01-26.csv',\n",
       " 'bydata/by-day/2011-01-27.csv',\n",
       " 'bydata/by-day/2011-01-28.csv',\n",
       " 'bydata/by-day/2011-01-30.csv',\n",
       " 'bydata/by-day/2011-01-31.csv',\n",
       " 'bydata/by-day/2011-02-01.csv',\n",
       " 'bydata/by-day/2011-02-02.csv',\n",
       " 'bydata/by-day/2011-02-03.csv',\n",
       " 'bydata/by-day/2011-02-04.csv',\n",
       " 'bydata/by-day/2011-02-06.csv',\n",
       " 'bydata/by-day/2011-02-07.csv',\n",
       " 'bydata/by-day/2011-02-08.csv',\n",
       " 'bydata/by-day/2011-02-09.csv',\n",
       " 'bydata/by-day/2011-02-10.csv',\n",
       " 'bydata/by-day/2011-02-11.csv',\n",
       " 'bydata/by-day/2011-02-13.csv',\n",
       " 'bydata/by-day/2011-02-14.csv',\n",
       " 'bydata/by-day/2011-02-15.csv',\n",
       " 'bydata/by-day/2011-02-16.csv',\n",
       " 'bydata/by-day/2011-02-17.csv',\n",
       " 'bydata/by-day/2011-02-18.csv',\n",
       " 'bydata/by-day/2011-02-20.csv',\n",
       " 'bydata/by-day/2011-02-21.csv',\n",
       " 'bydata/by-day/2011-02-22.csv',\n",
       " 'bydata/by-day/2011-02-23.csv',\n",
       " 'bydata/by-day/2011-02-24.csv',\n",
       " 'bydata/by-day/2011-02-25.csv',\n",
       " 'bydata/by-day/2011-02-27.csv',\n",
       " 'bydata/by-day/2011-02-28.csv',\n",
       " 'bydata/by-day/2011-03-01.csv',\n",
       " 'bydata/by-day/2011-03-02.csv',\n",
       " 'bydata/by-day/2011-03-03.csv',\n",
       " 'bydata/by-day/2011-03-04.csv',\n",
       " 'bydata/by-day/2011-03-06.csv',\n",
       " 'bydata/by-day/2011-03-07.csv',\n",
       " 'bydata/by-day/2011-03-08.csv',\n",
       " 'bydata/by-day/2011-03-09.csv',\n",
       " 'bydata/by-day/2011-03-10.csv',\n",
       " 'bydata/by-day/2011-03-11.csv',\n",
       " 'bydata/by-day/2011-03-13.csv',\n",
       " 'bydata/by-day/2011-03-14.csv',\n",
       " 'bydata/by-day/2011-03-15.csv',\n",
       " 'bydata/by-day/2011-03-16.csv',\n",
       " 'bydata/by-day/2011-03-17.csv',\n",
       " 'bydata/by-day/2011-03-18.csv',\n",
       " 'bydata/by-day/2011-03-20.csv',\n",
       " 'bydata/by-day/2011-03-21.csv',\n",
       " 'bydata/by-day/2011-03-22.csv',\n",
       " 'bydata/by-day/2011-03-23.csv',\n",
       " 'bydata/by-day/2011-03-24.csv',\n",
       " 'bydata/by-day/2011-03-25.csv',\n",
       " 'bydata/by-day/2011-03-27.csv',\n",
       " 'bydata/by-day/2011-03-28.csv',\n",
       " 'bydata/by-day/2011-03-29.csv',\n",
       " 'bydata/by-day/2011-03-30.csv',\n",
       " 'bydata/by-day/2011-03-31.csv',\n",
       " 'bydata/by-day/2011-04-01.csv',\n",
       " 'bydata/by-day/2011-04-03.csv',\n",
       " 'bydata/by-day/2011-04-04.csv',\n",
       " 'bydata/by-day/2011-04-05.csv',\n",
       " 'bydata/by-day/2011-04-06.csv',\n",
       " 'bydata/by-day/2011-04-07.csv',\n",
       " 'bydata/by-day/2011-04-08.csv',\n",
       " 'bydata/by-day/2011-04-10.csv',\n",
       " 'bydata/by-day/2011-04-11.csv',\n",
       " 'bydata/by-day/2011-04-12.csv',\n",
       " 'bydata/by-day/2011-04-13.csv',\n",
       " 'bydata/by-day/2011-04-14.csv',\n",
       " 'bydata/by-day/2011-04-15.csv',\n",
       " 'bydata/by-day/2011-04-17.csv',\n",
       " 'bydata/by-day/2011-04-18.csv',\n",
       " 'bydata/by-day/2011-04-19.csv',\n",
       " 'bydata/by-day/2011-04-20.csv',\n",
       " 'bydata/by-day/2011-04-21.csv',\n",
       " 'bydata/by-day/2011-04-26.csv',\n",
       " 'bydata/by-day/2011-04-27.csv',\n",
       " 'bydata/by-day/2011-04-28.csv',\n",
       " 'bydata/by-day/2011-05-01.csv',\n",
       " 'bydata/by-day/2011-05-03.csv',\n",
       " 'bydata/by-day/2011-05-04.csv',\n",
       " 'bydata/by-day/2011-05-05.csv',\n",
       " 'bydata/by-day/2011-05-06.csv',\n",
       " 'bydata/by-day/2011-05-08.csv',\n",
       " 'bydata/by-day/2011-05-09.csv',\n",
       " 'bydata/by-day/2011-05-10.csv',\n",
       " 'bydata/by-day/2011-05-11.csv',\n",
       " 'bydata/by-day/2011-05-12.csv',\n",
       " 'bydata/by-day/2011-05-13.csv',\n",
       " 'bydata/by-day/2011-05-15.csv',\n",
       " 'bydata/by-day/2011-05-16.csv',\n",
       " 'bydata/by-day/2011-05-17.csv',\n",
       " 'bydata/by-day/2011-05-18.csv',\n",
       " 'bydata/by-day/2011-05-19.csv',\n",
       " 'bydata/by-day/2011-05-20.csv',\n",
       " 'bydata/by-day/2011-05-22.csv',\n",
       " 'bydata/by-day/2011-05-23.csv',\n",
       " 'bydata/by-day/2011-05-24.csv',\n",
       " 'bydata/by-day/2011-05-25.csv',\n",
       " 'bydata/by-day/2011-05-26.csv',\n",
       " 'bydata/by-day/2011-05-27.csv',\n",
       " 'bydata/by-day/2011-05-29.csv',\n",
       " 'bydata/by-day/2011-05-31.csv',\n",
       " 'bydata/by-day/2011-06-01.csv',\n",
       " 'bydata/by-day/2011-06-02.csv',\n",
       " 'bydata/by-day/2011-06-03.csv',\n",
       " 'bydata/by-day/2011-06-05.csv',\n",
       " 'bydata/by-day/2011-06-06.csv',\n",
       " 'bydata/by-day/2011-06-07.csv',\n",
       " 'bydata/by-day/2011-06-08.csv',\n",
       " 'bydata/by-day/2011-06-09.csv',\n",
       " 'bydata/by-day/2011-06-10.csv',\n",
       " 'bydata/by-day/2011-06-12.csv',\n",
       " 'bydata/by-day/2011-06-13.csv',\n",
       " 'bydata/by-day/2011-06-14.csv',\n",
       " 'bydata/by-day/2011-06-15.csv',\n",
       " 'bydata/by-day/2011-06-16.csv',\n",
       " 'bydata/by-day/2011-06-17.csv',\n",
       " 'bydata/by-day/2011-06-19.csv',\n",
       " 'bydata/by-day/2011-06-20.csv',\n",
       " 'bydata/by-day/2011-06-21.csv',\n",
       " 'bydata/by-day/2011-06-22.csv',\n",
       " 'bydata/by-day/2011-06-23.csv',\n",
       " 'bydata/by-day/2011-06-24.csv',\n",
       " 'bydata/by-day/2011-06-26.csv',\n",
       " 'bydata/by-day/2011-06-27.csv',\n",
       " 'bydata/by-day/2011-06-28.csv',\n",
       " 'bydata/by-day/2011-06-29.csv',\n",
       " 'bydata/by-day/2011-06-30.csv',\n",
       " 'bydata/by-day/2011-07-01.csv',\n",
       " 'bydata/by-day/2011-07-03.csv',\n",
       " 'bydata/by-day/2011-07-04.csv',\n",
       " 'bydata/by-day/2011-07-05.csv',\n",
       " 'bydata/by-day/2011-07-06.csv',\n",
       " 'bydata/by-day/2011-07-07.csv',\n",
       " 'bydata/by-day/2011-07-08.csv',\n",
       " 'bydata/by-day/2011-07-10.csv',\n",
       " 'bydata/by-day/2011-07-11.csv',\n",
       " 'bydata/by-day/2011-07-12.csv',\n",
       " 'bydata/by-day/2011-07-13.csv',\n",
       " 'bydata/by-day/2011-07-14.csv',\n",
       " 'bydata/by-day/2011-07-15.csv',\n",
       " 'bydata/by-day/2011-07-17.csv',\n",
       " 'bydata/by-day/2011-07-18.csv',\n",
       " 'bydata/by-day/2011-07-19.csv',\n",
       " 'bydata/by-day/2011-07-20.csv',\n",
       " 'bydata/by-day/2011-07-21.csv',\n",
       " 'bydata/by-day/2011-07-22.csv',\n",
       " 'bydata/by-day/2011-07-24.csv',\n",
       " 'bydata/by-day/2011-07-25.csv',\n",
       " 'bydata/by-day/2011-07-26.csv',\n",
       " 'bydata/by-day/2011-07-27.csv',\n",
       " 'bydata/by-day/2011-07-28.csv',\n",
       " 'bydata/by-day/2011-07-29.csv',\n",
       " 'bydata/by-day/2011-07-31.csv',\n",
       " 'bydata/by-day/2011-08-01.csv',\n",
       " 'bydata/by-day/2011-08-02.csv',\n",
       " 'bydata/by-day/2011-08-03.csv',\n",
       " 'bydata/by-day/2011-08-04.csv',\n",
       " 'bydata/by-day/2011-08-05.csv',\n",
       " 'bydata/by-day/2011-08-07.csv',\n",
       " 'bydata/by-day/2011-08-08.csv',\n",
       " 'bydata/by-day/2011-08-09.csv',\n",
       " 'bydata/by-day/2011-08-10.csv',\n",
       " 'bydata/by-day/2011-08-11.csv',\n",
       " 'bydata/by-day/2011-08-12.csv',\n",
       " 'bydata/by-day/2011-08-14.csv',\n",
       " 'bydata/by-day/2011-08-15.csv',\n",
       " 'bydata/by-day/2011-08-16.csv',\n",
       " 'bydata/by-day/2011-08-17.csv',\n",
       " 'bydata/by-day/2011-08-18.csv',\n",
       " 'bydata/by-day/2011-08-19.csv',\n",
       " 'bydata/by-day/2011-08-21.csv',\n",
       " 'bydata/by-day/2011-08-22.csv',\n",
       " 'bydata/by-day/2011-08-23.csv',\n",
       " 'bydata/by-day/2011-08-24.csv',\n",
       " 'bydata/by-day/2011-08-25.csv',\n",
       " 'bydata/by-day/2011-08-26.csv',\n",
       " 'bydata/by-day/2011-08-28.csv',\n",
       " 'bydata/by-day/2011-08-30.csv',\n",
       " 'bydata/by-day/2011-08-31.csv',\n",
       " 'bydata/by-day/2011-09-01.csv',\n",
       " 'bydata/by-day/2011-09-02.csv',\n",
       " 'bydata/by-day/2011-09-04.csv',\n",
       " 'bydata/by-day/2011-09-05.csv',\n",
       " 'bydata/by-day/2011-09-06.csv',\n",
       " 'bydata/by-day/2011-09-07.csv',\n",
       " 'bydata/by-day/2011-09-08.csv',\n",
       " 'bydata/by-day/2011-09-09.csv',\n",
       " 'bydata/by-day/2011-09-11.csv',\n",
       " 'bydata/by-day/2011-09-12.csv',\n",
       " 'bydata/by-day/2011-09-13.csv',\n",
       " 'bydata/by-day/2011-09-14.csv',\n",
       " 'bydata/by-day/2011-09-15.csv',\n",
       " 'bydata/by-day/2011-09-16.csv',\n",
       " 'bydata/by-day/2011-09-18.csv',\n",
       " 'bydata/by-day/2011-09-19.csv',\n",
       " 'bydata/by-day/2011-09-20.csv',\n",
       " 'bydata/by-day/2011-09-21.csv',\n",
       " 'bydata/by-day/2011-09-22.csv',\n",
       " 'bydata/by-day/2011-09-23.csv',\n",
       " 'bydata/by-day/2011-09-25.csv',\n",
       " 'bydata/by-day/2011-09-26.csv',\n",
       " 'bydata/by-day/2011-09-27.csv',\n",
       " 'bydata/by-day/2011-09-28.csv',\n",
       " 'bydata/by-day/2011-09-29.csv',\n",
       " 'bydata/by-day/2011-09-30.csv',\n",
       " 'bydata/by-day/2011-10-02.csv',\n",
       " 'bydata/by-day/2011-10-03.csv',\n",
       " 'bydata/by-day/2011-10-04.csv',\n",
       " 'bydata/by-day/2011-10-05.csv',\n",
       " 'bydata/by-day/2011-10-06.csv',\n",
       " 'bydata/by-day/2011-10-07.csv',\n",
       " 'bydata/by-day/2011-10-09.csv',\n",
       " 'bydata/by-day/2011-10-10.csv',\n",
       " 'bydata/by-day/2011-10-11.csv',\n",
       " 'bydata/by-day/2011-10-12.csv',\n",
       " 'bydata/by-day/2011-10-13.csv',\n",
       " 'bydata/by-day/2011-10-14.csv',\n",
       " 'bydata/by-day/2011-10-16.csv',\n",
       " 'bydata/by-day/2011-10-17.csv',\n",
       " 'bydata/by-day/2011-10-18.csv',\n",
       " 'bydata/by-day/2011-10-19.csv',\n",
       " 'bydata/by-day/2011-10-20.csv',\n",
       " 'bydata/by-day/2011-10-21.csv',\n",
       " 'bydata/by-day/2011-10-23.csv',\n",
       " 'bydata/by-day/2011-10-24.csv',\n",
       " 'bydata/by-day/2011-10-25.csv',\n",
       " 'bydata/by-day/2011-10-26.csv',\n",
       " 'bydata/by-day/2011-10-27.csv',\n",
       " 'bydata/by-day/2011-10-28.csv',\n",
       " 'bydata/by-day/2011-10-30.csv',\n",
       " 'bydata/by-day/2011-10-31.csv',\n",
       " 'bydata/by-day/2011-11-01.csv',\n",
       " 'bydata/by-day/2011-11-02.csv',\n",
       " 'bydata/by-day/2011-11-03.csv',\n",
       " 'bydata/by-day/2011-11-04.csv',\n",
       " 'bydata/by-day/2011-11-06.csv',\n",
       " 'bydata/by-day/2011-11-07.csv',\n",
       " 'bydata/by-day/2011-11-08.csv',\n",
       " 'bydata/by-day/2011-11-09.csv',\n",
       " 'bydata/by-day/2011-11-10.csv',\n",
       " 'bydata/by-day/2011-11-11.csv',\n",
       " 'bydata/by-day/2011-11-13.csv',\n",
       " 'bydata/by-day/2011-11-14.csv',\n",
       " 'bydata/by-day/2011-11-15.csv',\n",
       " 'bydata/by-day/2011-11-16.csv',\n",
       " 'bydata/by-day/2011-11-17.csv',\n",
       " 'bydata/by-day/2011-11-18.csv',\n",
       " 'bydata/by-day/2011-11-20.csv',\n",
       " 'bydata/by-day/2011-11-21.csv',\n",
       " 'bydata/by-day/2011-11-22.csv',\n",
       " 'bydata/by-day/2011-11-23.csv',\n",
       " 'bydata/by-day/2011-11-24.csv',\n",
       " 'bydata/by-day/2011-11-25.csv',\n",
       " 'bydata/by-day/2011-11-27.csv',\n",
       " 'bydata/by-day/2011-11-28.csv',\n",
       " 'bydata/by-day/2011-11-29.csv',\n",
       " 'bydata/by-day/2011-11-30.csv',\n",
       " 'bydata/by-day/2011-12-01.csv',\n",
       " 'bydata/by-day/2011-12-02.csv',\n",
       " 'bydata/by-day/2011-12-04.csv',\n",
       " 'bydata/by-day/2011-12-05.csv',\n",
       " 'bydata/by-day/2011-12-06.csv',\n",
       " 'bydata/by-day/2011-12-07.csv',\n",
       " 'bydata/by-day/2011-12-08.csv',\n",
       " 'bydata/by-day/2011-12-09.csv']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(\"bydata/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "07191a6e-def8-4828-9900-ef014f656cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한개의 csv파일을 읽어들여서 RDD로 변경하는 함수\n",
    "def read_csv_to_rdd(file_path, header=None):\n",
    "    data = spark.sparkContext.textFile(file_path)\n",
    "    if header == None :\n",
    "        header = data.first()\n",
    "        data = csv_lines.filter(lambda line : line != header)\n",
    "    return data\n",
    "# 합치는 함수\n",
    "def merge_csv_files_folers(csv_file_lists):\n",
    "    csv_file_lists = glob(filepaths)\n",
    "    return spark.sparkContext.union( [ read_csv_to_rdd(csvfile) for csvfile in csv_file_lists ] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a6d148df-4743-4ef7-86cb-94ffaf37bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = \"bydata/by-day/*.csv\"\n",
    "all_csv_rdd = merge_csv_files_folers(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8f79ffc4-59a4-47e2-a589-0b4cf1cbed0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_csv_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "44eac757-9dd8-4327-9346-bc153ab591e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "542214"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_csv_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c84b9437-3b48-4c7d-9717-0d06f091f48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['536381,21523,DOORMAT FANCY FONT HOME SWEET HOME,10,2010-12-01 09:41:00,6.75,15311.0,United Kingdom',\n",
       " '536388,22464,HANGING METAL HEART LANTERN,12,2010-12-01 09:59:00,1.65,16250.0,United Kingdom']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_csv_rdd.distinct().collect())\n",
    "distinct_all_csv_rdd = all_csv_rdd.distinct()\n",
    "distinct_all_csv_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed637bd4-97ff-4087-8f3d-45f0edecbae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "542214"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_csv_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11574c58-0eff-431e-9c01-7436fb1d4ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "542214"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파일이 있는 경로만 설정해도 파일 리딩 가능\n",
    "new_files = \"bydata/by-day/*.csv\"\n",
    "new_read_csv = spark.sparkContext.textFile(new_files)\n",
    "len(new_read_csv.collect() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3ab92d-4757-4ffb-8b58-c8d82fa80edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advenced RDD\n",
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109e86ea-0b33-4856-abfa-439d6ceea12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = spark.sparkContext.parallelize(myCollection,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2904fa6d-7fab-4957-9e81-b85b68f2818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark', 1),\n",
       " ('the', 1),\n",
       " ('definitive', 1),\n",
       " ('guide', 1),\n",
       " (':', 1),\n",
       " ('big', 1),\n",
       " ('data', 1),\n",
       " ('processing', 1),\n",
       " ('made', 1),\n",
       " ('simple', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda w : (w.lower(),1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be0e3cb-a6b6-4ca7-8e49-3458263a7829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 'Spark'),\n",
       " ('t', 'The'),\n",
       " ('d', 'Definitive'),\n",
       " ('g', 'Guide'),\n",
       " (':', ':'),\n",
       " ('b', 'Big'),\n",
       " ('d', 'Data'),\n",
       " ('p', 'Processing'),\n",
       " ('m', 'Made'),\n",
       " ('s', 'Simple')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.keyBy(lambda w:w.lower()[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b860dfb3-42e1-4fbe-aea5-59d82f4cae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = words.keyBy(lambda w : w.lower()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5a177d-64a9-4e42-ad6b-f82c6a3d91dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 'SPARK'),\n",
       " ('t', 'THE'),\n",
       " ('d', 'DEFINITIVE'),\n",
       " ('g', 'GUIDE'),\n",
       " (':', ':'),\n",
       " ('b', 'BIG'),\n",
       " ('d', 'DATA'),\n",
       " ('p', 'PROCESSING'),\n",
       " ('m', 'MADE'),\n",
       " ('s', 'SIMPLE')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.mapValues(lambda w:w.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5fa1de-4644-4d30-82d8-d6c2d0c254bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 'S'),\n",
       " ('s', 'P'),\n",
       " ('s', 'A'),\n",
       " ('s', 'R'),\n",
       " ('s', 'K'),\n",
       " ('t', 'T'),\n",
       " ('t', 'H'),\n",
       " ('t', 'E'),\n",
       " ('d', 'D'),\n",
       " ('d', 'E'),\n",
       " ('d', 'F'),\n",
       " ('d', 'I'),\n",
       " ('d', 'N'),\n",
       " ('d', 'I'),\n",
       " ('d', 'T'),\n",
       " ('d', 'I'),\n",
       " ('d', 'V'),\n",
       " ('d', 'E'),\n",
       " ('g', 'G'),\n",
       " ('g', 'U'),\n",
       " ('g', 'I'),\n",
       " ('g', 'D'),\n",
       " ('g', 'E'),\n",
       " (':', ':'),\n",
       " ('b', 'B'),\n",
       " ('b', 'I'),\n",
       " ('b', 'G'),\n",
       " ('d', 'D'),\n",
       " ('d', 'A'),\n",
       " ('d', 'T'),\n",
       " ('d', 'A'),\n",
       " ('p', 'P'),\n",
       " ('p', 'R'),\n",
       " ('p', 'O'),\n",
       " ('p', 'C'),\n",
       " ('p', 'E'),\n",
       " ('p', 'S'),\n",
       " ('p', 'S'),\n",
       " ('p', 'I'),\n",
       " ('p', 'N'),\n",
       " ('p', 'G'),\n",
       " ('m', 'M'),\n",
       " ('m', 'A'),\n",
       " ('m', 'D'),\n",
       " ('m', 'E'),\n",
       " ('s', 'S'),\n",
       " ('s', 'I'),\n",
       " ('s', 'M'),\n",
       " ('s', 'P'),\n",
       " ('s', 'L'),\n",
       " ('s', 'E')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.flatMapValues(lambda w:w.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b1d53d-e9bc-4890-8433-9365bc6639b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('s', 'Spark'), ('t', 'The')], PythonRDD[6] at RDD at PythonRDD.scala:53)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.take(2), keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fcb16c3-fc9d-49e7-acf3-f1a6f5ada54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['s', 't', 'd', 'g', ':', 'b', 'd', 'p', 'm', 's'],\n",
       " ['Spark',\n",
       "  'The',\n",
       "  'Definitive',\n",
       "  'Guide',\n",
       "  ':',\n",
       "  'Big',\n",
       "  'Data',\n",
       "  'Processing',\n",
       "  'Made',\n",
       "  'Simple'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.keys().collect(), keyword.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473e93b0-7009-4a80-a056-6cdf4e46373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어서 소문자로 변환된 문자들을 추출\n",
    "# 중복된 문자을을 제거\n",
    "# 각 문자에서 무작위로 생성된 값을 가지는 샘플을 생성\n",
    "# 각 단어의 첫 문자를 키로 하고 단어를 값으로 하는 키-값 쌍을 생성\n",
    "# sampleBykey() 함수를 사용하여 각 키에 대한 샘플을 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ef4426-cc8e-4272-9737-156af71df35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4418fdfd-f4d1-4e7a-a433-88ff8176f0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'the',\n",
       " 'definitive',\n",
       " 'guide',\n",
       " ':',\n",
       " 'big',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'made',\n",
       " 'simple']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda x : x.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5497c9bf-f16a-4fe3-8296-d3e4592026e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctChar = words.flatMap(lambda x : x.lower()).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a71e5350-fdc1-463d-bee6-a45a07f7388d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 'The'), ('t', 'The'), ('m', 'Made')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleMap = dict( map(lambda c: (c, random.random()),distinctChar) )\n",
    "words.map(lambda w:(w.lower()[0],w)).sampleByKey(True,sampleMap,6).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec10836b-4412-4330-93af-8afa76105782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'p']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words 단어리스트를 소문자로 변환한 후에 각 문자를 추출\n",
    "chars = words.flatMap(lambda w:w.lower())\n",
    "chars.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56655630-026d-4902-b8a6-b5a7a8bac447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 1), ('p', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각문자를 (문자,1)형태로 키,값 변환\n",
    "dic_chars = chars.map(lambda w:(w,1))\n",
    "dic_chars.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e835828-d5e7-4c20-a155-eed57774c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxFunc(left,right):\n",
    "    return max(left, right)\n",
    "def addFunc(left,right):\n",
    "    return left+right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca9750c5-bb86-4948-8b6c-9950c251bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = spark.sparkContext.parallelize(range(1,31),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3312ca3a-3d45-4ac1-bcd4-db0a3091e70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 4),\n",
       " ('p', 3),\n",
       " ('r', 2),\n",
       " ('h', 1),\n",
       " ('d', 4),\n",
       " ('i', 7),\n",
       " ('g', 3),\n",
       " ('b', 1),\n",
       " ('c', 1),\n",
       " ('l', 1),\n",
       " ('a', 4),\n",
       " ('k', 1),\n",
       " ('t', 3),\n",
       " ('e', 7),\n",
       " ('f', 1),\n",
       " ('n', 2),\n",
       " ('v', 1),\n",
       " ('u', 1),\n",
       " (':', 1),\n",
       " ('o', 1),\n",
       " ('m', 2)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict_chars를 이용하여 각 문자의 출현 빈도를 계산\n",
    "dic_chars.reduceByKey(lambda x,y : x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4942a9c-c129-42a4-994f-e41491138107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value : 30\n",
      "total_sum : 465\n"
     ]
    }
   ],
   "source": [
    "# num 에서 최대값과\n",
    "max_value = nums.reduce(maxFunc)\n",
    "print(f'max value : {max_value}')\n",
    "\n",
    "# num에서 모든 값의 합\n",
    "total_sum = nums.reduce(addFunc)\n",
    "print(f'total_sum : {total_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b188fbb5-674a-4e7c-a68a-24bbd9c87edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'s': 4,\n",
       "             'p': 3,\n",
       "             'a': 4,\n",
       "             'r': 2,\n",
       "             'k': 1,\n",
       "             't': 3,\n",
       "             'h': 1,\n",
       "             'e': 7,\n",
       "             'd': 4,\n",
       "             'f': 1,\n",
       "             'i': 7,\n",
       "             'n': 2,\n",
       "             'v': 1,\n",
       "             'g': 3,\n",
       "             'u': 1,\n",
       "             ':': 1,\n",
       "             'b': 1,\n",
       "             'o': 1,\n",
       "             'c': 1,\n",
       "             'm': 2,\n",
       "             'l': 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_chars.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a90ba16-f85f-40cb-9cc8-422ec4a11600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('s', 4),\n",
       " ('p', 3),\n",
       " ('r', 2),\n",
       " ('h', 1),\n",
       " ('d', 4),\n",
       " ('i', 7),\n",
       " ('g', 3),\n",
       " ('b', 1),\n",
       " ('c', 1),\n",
       " ('l', 1),\n",
       " ('a', 4),\n",
       " ('k', 1),\n",
       " ('t', 3),\n",
       " ('e', 7),\n",
       " ('f', 1),\n",
       " ('n', 2),\n",
       " ('v', 1),\n",
       " ('u', 1),\n",
       " (':', 1),\n",
       " ('o', 1),\n",
       " ('m', 2)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "dic_chars.groupByKey().map(lambda row : (row[0], reduce(addFunc,row[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fc779fe-a1cb-4a6c-8dc0-9954f58892f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금 3개의 코드가 같은 결과ㄹ 내고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ffd6974-1bfa-43a5-9c80-20ce83c43122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.aggregate(0,maxFunc,addFunc) # 초기값은 0, 파티션 별로 시퀀스 연산 (maxFunc)를 하고 그결과를 병합 연산(addFunc)\n",
    "# 모든 파티션의 최대값의 합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5bd1d0a-bd0f-4958-b532-0ff9bb925007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 피티션이 분할되어 있어서\n",
    "# RDD의 요소를 병렬로 집계\n",
    "depth = 3\n",
    "nums.treeAggregate(0,maxFunc,addFunc,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb62c82d-85a4-47ee-a518-282296761fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 3),\n",
       " ('p', 2),\n",
       " ('r', 1),\n",
       " ('h', 1),\n",
       " ('d', 2),\n",
       " ('i', 4),\n",
       " ('g', 2),\n",
       " ('b', 1),\n",
       " ('c', 1),\n",
       " ('l', 1),\n",
       " ('a', 3),\n",
       " ('k', 1),\n",
       " ('t', 2),\n",
       " ('e', 4),\n",
       " ('f', 1),\n",
       " ('n', 1),\n",
       " ('v', 1),\n",
       " ('u', 1),\n",
       " (':', 1),\n",
       " ('o', 1),\n",
       " ('m', 2)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD의 키별로 값을 집계하는 역할\n",
    "dic_chars.aggregateByKey(0,addFunc,maxFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6805a3d-8747-4bb3-88bd-7d4fdf950cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv파일르 로드하고 전처리해서 parquet으로 저장하기\n",
    "# mysql write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c9972e8-08f9-47ed-9fd3-d7774abbead1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-25 15:40:39--  https://github.com/kyuyounglee/encore_DE30/blob/main/data/doc_log.csv\n",
      "Resolving github.com (github.com)... 20.200.245.247\n",
      "Connecting to github.com (github.com)|20.200.245.247|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: `doc_log.csv'\n",
      "\n",
      "doc_log.csv             [ <=>                ] 142.31K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-03-25 15:40:39 (2.38 MB/s) - `doc_log.csv' saved [145721]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://github.com/kyuyounglee/encore_DE30/blob/main/data/doc_log.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76c1778b-93c6-48ac-ac32-09f77d33f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "814aac81-8983-44bd-b1e8-639aee67d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+--------------------+----------------+---------+\n",
      "|actiontype|ismydoc| ext|           sessionid|documentposition| datetime|\n",
      "+----------+-------+----+--------------------+----------------+---------+\n",
      "|      OPEN|  false| PDF|9400fd2e43d7dc2d0...|    LOCALSTORAGE|2016.7.18|\n",
      "|     CLOSE|  false| PDF|9400fd2e43d7dc2d0...|    LOCALSTORAGE|2016.7.18|\n",
      "|      OPEN|   true| PDF|9400fd2e43d7dc2d0...|  MYPOLARISDRIVE|2016.7.18|\n",
      "|     CLOSE|   true| PDF|9400fd2e43d7dc2d0...|  MYPOLARISDRIVE|2016.7.18|\n",
      "|      OPEN|  false| PDF|f191063c562691041...|        OTHERAPP| 2016.7.6|\n",
      "|     RESET|   true| PDF|f191063c562691041...|        OTHERAPP| 2016.7.6|\n",
      "|     CLOSE|  false| PDF|3747719d8f6bdd334...|        OTHERAPP|2016.7.20|\n",
      "|      OPEN|  false| PDF|3747719d8f6bdd334...|        OTHERAPP|2016.7.20|\n",
      "|      OPEN|  false| PDF|3da5ab986c93803de...|        OTHERAPP|2016.7.28|\n",
      "|     CLOSE|  false| PDF|3da5ab986c93803de...|        OTHERAPP|2016.7.28|\n",
      "|      OPEN|   true| PDF|3da5ab986c93803de...|        OTHERAPP|2016.7.28|\n",
      "|      OPEN|  false| PDF|9e37751e132b5eb96...|        OTHERAPP|2016.7.19|\n",
      "|     CLOSE|  false| PDF|9e37751e132b5eb96...|        OTHERAPP|2016.7.19|\n",
      "|     RESET|   true| PDF|9e37751e132b5eb96...|        OTHERAPP|2016.7.19|\n",
      "|      OPEN|   true| PDF|450e74586e6a05f29...|  MYPOLARISDRIVE|2016.7.24|\n",
      "|     CLOSE|   true| PDF|450e74586e6a05f29...|  MYPOLARISDRIVE|2016.7.24|\n",
      "|      OPEN|  false| PDF|202c847b75c9920eb...|        OTHERAPP|2016.7.24|\n",
      "|     RESET|   true| HWP|897f9d042720251e2...|        OTHERAPP|2016.7.11|\n",
      "|      OPEN|  false| HWP|897f9d042720251e2...|        OTHERAPP|2016.7.11|\n",
      "|     RESET|   true|XLSX|897f9d042720251e2...|        OTHERAPP|2016.7.11|\n",
      "+----------+-------+----+--------------------+----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read csv file\n",
    "spark_df = spark.read.format(\"csv\").option('header','true').option('inferSchema','true').load('doc_log.csv')\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "243861e0-935d-43fb-8511-6aa76497adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시테이블로 등록 spark_df\n",
    "spark_df.createOrReplaceTempView('spark_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6dc4e34a-e7f0-4707-a06b-54939d68b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+--------------------------------+----------------+---------+\n",
      "|actiontype|ismydoc|ext |sessionid                       |documentposition|datetime |\n",
      "+----------+-------+----+--------------------------------+----------------+---------+\n",
      "|OPEN      |true   |PDF |9400fd2e43d7dc2d054ca78806236ee1|MYPOLARISDRIVE  |2016.7.18|\n",
      "|CLOSE     |true   |PDF |9400fd2e43d7dc2d054ca78806236ee1|MYPOLARISDRIVE  |2016.7.18|\n",
      "|RESET     |true   |PDF |f191063c562691041dfa935ff0876975|OTHERAPP        |2016.7.6 |\n",
      "|OPEN      |true   |PDF |3da5ab986c93803de1e25012d9972274|OTHERAPP        |2016.7.28|\n",
      "|RESET     |true   |PDF |9e37751e132b5eb96e7d3fde7db132e3|OTHERAPP        |2016.7.19|\n",
      "|OPEN      |true   |PDF |450e74586e6a05f2983905205e8f8cb1|MYPOLARISDRIVE  |2016.7.24|\n",
      "|CLOSE     |true   |PDF |450e74586e6a05f2983905205e8f8cb1|MYPOLARISDRIVE  |2016.7.24|\n",
      "|RESET     |true   |HWP |897f9d042720251e2d08bfefc3bd6ec9|OTHERAPP        |2016.7.11|\n",
      "|RESET     |true   |XLSX|897f9d042720251e2d08bfefc3bd6ec9|OTHERAPP        |2016.7.11|\n",
      "|RESET     |true   |PDF |7a6aa42733f82e72e6202ddf942eee32|OTHERAPP        |2016.7.22|\n",
      "+----------+-------+----+--------------------------------+----------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ismydoc = true\n",
    "df1 = spark.sql(\"select * from spark_df where ismydoc = true\")\n",
    "df1.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01da5d0d-edf7-4a12-a5d7-ba94c71c77f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/25 16:19:49 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "301861"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sessionid, ext 컬럼 중에서 ext = PDF 또는 ext == DOC 인 데이터들 중에서 중복데이터를 제거하고 캐쉬\n",
    "df2 = spark.sql('select * from spark_df')\n",
    "df2_pdf = df2.select('sessionid','ext').filter(\"ext=='PDF' or ext == 'DDC'\").dropDuplicates().cache()\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99821494-4afd-4700-b41c-c0e956b4c54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|           sessionid| min_date|\n",
      "+--------------------+---------+\n",
      "|0001625bdb4fb9136...|2016.7.19|\n",
      "|0001b27b377723145...|2016.7.20|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sessionid 별로 datetime의 최소값\n",
    "df2_min_date = df2.groupBy('sessionid').agg(min('datetime').alias('min_date'))\n",
    "df2_min_date.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a451f39f-dae4-4239-b264-099fe2809dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---------+\n",
      "|           sessionid|ext| min_date|\n",
      "+--------------------+---+---------+\n",
      "|551de498388693734...|PDF| 2016.7.9|\n",
      "|ffef6402dac05483f...|PDF|2016.7.12|\n",
      "+--------------------+---+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df2_pdf 를 마스터로 해서  df2_min_date 병합\n",
    "# df2_pdf 목록은 전부 나오게 하고 매칭 안되는 데이터는 na\n",
    "df2_join = df2_pdf.join(df2_min_date,'sessionid','left')\n",
    "df2_join.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1dc20bef-a27d-4217-ba0f-5bae1a0d70a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|sum(CAST((sessionid IS NULL) AS INT))|\n",
      "+-------------------------------------+\n",
      "|                                    0|\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 결측치 조사\n",
    "df2_join.select(sum(col('sessionid').isNull().cast('int') ) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19f66e6f-e452-438e-810c-222ddbab2b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---------+\n",
      "|           sessionid|ext| min_date|\n",
      "+--------------------+---+---------+\n",
      "|551de498388693734...|PDF| 2016.7.9|\n",
      "|ffef6402dac05483f...|PDF|2016.7.12|\n",
      "|635a5c8d3df7b0a40...|PDF|2016.7.15|\n",
      "|c389b7b211b044b56...|PDF|2016.7.22|\n",
      "|d5b91aaa2093e421a...|PDF| 2016.7.1|\n",
      "|204f6839bbe3e5504...|PDF|2016.7.15|\n",
      "|8c8fed61f21992f00...|PDF|2016.7.17|\n",
      "|10ad1c7d1d4f7f4ad...|PDF|2016.7.25|\n",
      "|33c6ef601e915c1a0...|PDF|2016.7.14|\n",
      "|a7d01eac986e2f8f1...|PDF| 2016.7.1|\n",
      "+--------------------+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_join.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e9bddbb-eef4-43ed-aecf-f0284c06d619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sessionid|\n",
      "+---------+\n",
      "|        0|\n",
      "+---------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|ext|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:====================================================> (193 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|min_date|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 모든 열에 대해서 결측치 여부 조사\n",
    "for col_name in df2_join.columns:\n",
    "    print( df2_join.select(sum(col(col_name).isNull().cast('int') ).alias(col_name) ).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5f8b4-0b4e-45c3-83cc-dfceeabb9441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
