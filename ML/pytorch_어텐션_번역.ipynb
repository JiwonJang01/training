{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "데이터 확인"
      ],
      "metadata": {
        "id": "ectfEN6ixDJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYvKS9wNsrIh",
        "outputId": "3c9c0cb6-dd80-4c4a-bf7b-8dba481d3dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/kor-eng.zip\n",
            "  inflating: koreng/_about.txt       \n",
            "  inflating: koreng/kor.txt          \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/kor-eng.zip' -d koreng"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "new_lines = []\n",
        "with open( '/content/koreng/kor.txt','r' ,encoding='utf-8' ) as f:\n",
        "  lines = f.read().split('\\n')\n",
        "  for line in lines:\n",
        "    try:\n",
        "      txt = \"\".join( v for v in line if v not in string.punctuation).lower()\n",
        "      txt = txt.split()\n",
        "      txt = txt[0]+'\\t'+txt[1]\n",
        "      new_lines.append(txt)\n",
        "    except IndexError as e:\n",
        "      print(txt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9YgAHi5xEqN",
        "outputId": "be82ed13-062c-4ac9-8430-e84ceec4a24c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_lines[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gnnJp4U0gz8",
        "outputId": "b5e4537e-cc92-49f9-da85-98468b1d70bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go\\t가', 'hi\\t안녕', 'run\\t뛰어', 'run\\t뛰어', 'who\\t누구']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습용 데이터 만들기"
      ],
      "metadata": {
        "id": "QFc0pR2n2wz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset"
      ],
      "metadata": {
        "id": "6VNI1vdl2v83"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장으로 부터 BOW\n",
        "def get_BOW(corpus):\n",
        "  BOW = {\"<SOS>\":0,\"<EOS>\":1}\n",
        "  for line in corpus:\n",
        "    for word in line.split():\n",
        "      if word not in BOW.keys():\n",
        "        BOW[word] = len(BOW.keys())\n",
        "  return BOW"
      ],
      "metadata": {
        "id": "tNtSriQGzcXF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Eng2Kor(Dataset):\n",
        "  def __init__(self, filepath = '/content/koreng/kor.txt'):\n",
        "    self.eng_corpus = [] # 영어 문장\n",
        "    self.kor_corpus = [] # 한글 문장\n",
        "    # 파일에서 데이터 읽음\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "      lines = f.read().split('\\n')\n",
        "      for line in lines:\n",
        "        try:\n",
        "          # 특수문자 제거, 소문자로 변경\n",
        "          txt = \"\".join(v for v in line if v not in string.punctuation).lower()\n",
        "          txt = txt.split(\"\\t\")\n",
        "          # 길이가 10이하인 문장만 사용\n",
        "          engtxt = txt[0]; kortxt = txt[1]\n",
        "          if len(engtxt) <=10 and len(kortxt) <=10 :\n",
        "            self.eng_corpus.append(engtxt)\n",
        "            self.kor_corpus.append(kortxt)\n",
        "        except Exception as e:\n",
        "          print(e, txt)\n",
        "\n",
        "    self.engBOW = get_BOW(self.eng_corpus)\n",
        "    self.korBOW = get_BOW(self.kor_corpus)\n",
        "  def __len__(self):\n",
        "    return len(self.engBOW)\n",
        "\n",
        "  # 문장을 단어로 분리하고 마지막에 <EOS> 추가\n",
        "  def get_seq(self, line):\n",
        "    seq = line.split()\n",
        "    seq.append(\"<EOS>\")\n",
        "    return seq\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # 문자열을 숫자로 변환\n",
        "    # 영어, 한국어\n",
        "    data = np.array([ self.engBOW[txt] for txt in self.get_seq(self.eng_corpus[index]) ])\n",
        "    label = np.array([ self.korBOW[txt] for txt in self.get_seq(self.kor_corpus[index]) ])\n",
        "    return data,label"
      ],
      "metadata": {
        "id": "WPtB5FuY3zNm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Eng2Kor()\n",
        "data,label = next(iter(dataset))\n",
        "data, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JDKL9Zv9-Cn",
        "outputId": "33f9c586-93fa-4756-df6d-636794878d4d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list index out of range ['']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2, 1]), array([2, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loader(dataset):\n",
        "  for i in range(len(dataset)):\n",
        "    data,label = dataset[i]\n",
        "    yield torch.tensor(data), torch.tensor(label)"
      ],
      "metadata": {
        "id": "fpRngrOJ-lym"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data, label in loader(dataset):\n",
        "  print(data,label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW8XODKeDJOO",
        "outputId": "579896e9-7928-4a57-ce29-2424690c30d4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 1]) tensor([2, 1])\n",
            "tensor([3, 1]) tensor([3, 1])\n",
            "tensor([4, 1]) tensor([4, 1])\n",
            "tensor([4, 1]) tensor([4, 1])\n",
            "tensor([5, 1]) tensor([5, 1])\n",
            "tensor([6, 1]) tensor([6, 1])\n",
            "tensor([7, 1]) tensor([7, 1])\n",
            "tensor([8, 1]) tensor([8, 1])\n",
            "tensor([9, 1]) tensor([9, 1])\n",
            "tensor([10,  1]) tensor([10,  1])\n",
            "tensor([11,  1]) tensor([11,  1])\n",
            "tensor([11,  1]) tensor([12,  1])\n",
            "tensor([12,  1]) tensor([13, 14,  1])\n",
            "tensor([13,  1]) tensor([15,  1])\n",
            "tensor([13,  1]) tensor([16,  1])\n",
            "tensor([13,  1]) tensor([15,  1])\n",
            "tensor([14,  1]) tensor([17,  1])\n",
            "tensor([15,  1]) tensor([3, 1])\n",
            "tensor([15,  1]) tensor([3, 1])\n",
            "tensor([16, 17,  1]) tensor([18,  1])\n",
            "tensor([16, 18,  1]) tensor([19,  1])\n",
            "tensor([16, 19,  1]) tensor([20, 21,  1])\n",
            "tensor([20, 21,  1]) tensor([22, 23,  1])\n",
            "tensor([22,  1]) tensor([24,  1])\n",
            "tensor([23,  1]) tensor([8, 1])\n",
            "tensor([24,  1]) tensor([25,  1])\n",
            "tensor([25,  1]) tensor([26,  1])\n",
            "tensor([25,  1]) tensor([27, 28, 29,  1])\n",
            "tensor([25,  1]) tensor([30, 31,  1])\n",
            "tensor([26,  1]) tensor([32,  1])\n",
            "tensor([26,  1]) tensor([33,  1])\n",
            "tensor([27,  1]) tensor([34,  1])\n",
            "tensor([28, 29,  1]) tensor([35,  1])\n",
            "tensor([30, 31,  1]) tensor([36,  1])\n",
            "tensor([30, 31,  1]) tensor([36,  1])\n",
            "tensor([32, 33,  1]) tensor([37, 38,  1])\n",
            "tensor([34, 35,  1]) tensor([39,  1])\n",
            "tensor([16, 36,  1]) tensor([40,  1])\n",
            "tensor([16, 37,  1]) tensor([41, 42,  1])\n",
            "tensor([16, 38,  1]) tensor([43, 44,  1])\n",
            "tensor([16, 39,  1]) tensor([43, 45,  1])\n",
            "tensor([40,  1]) tensor([46,  1])\n",
            "tensor([21, 41,  1]) tensor([47, 48,  1])\n",
            "tensor([21, 41,  1]) tensor([49,  1])\n",
            "tensor([42,  1]) tensor([50,  1])\n",
            "tensor([43, 18,  1]) tensor([51, 52,  1])\n",
            "tensor([43, 19,  1]) tensor([53, 21,  1])\n",
            "tensor([44, 35,  1]) tensor([54, 55,  1])\n",
            "tensor([45,  1]) tensor([56,  1])\n",
            "tensor([46, 47,  1]) tensor([57,  1])\n",
            "tensor([46, 47,  1]) tensor([13, 14,  1])\n",
            "tensor([46, 48,  1]) tensor([58, 59,  1])\n",
            "tensor([49, 31,  1]) tensor([60,  2,  1])\n",
            "tensor([50, 51,  1]) tensor([61, 62,  1])\n",
            "tensor([52, 53,  1]) tensor([63,  1])\n",
            "tensor([52, 54,  1]) tensor([64,  1])\n",
            "tensor([28, 55,  1]) tensor([65,  1])\n",
            "tensor([ 2, 56,  1]) tensor([60,  2,  1])\n",
            "tensor([ 2, 56,  1]) tensor([60,  2,  1])\n",
            "tensor([57,  1]) tensor([3, 1])\n",
            "tensor([57,  1]) tensor([66, 67,  1])\n",
            "tensor([32, 58,  1]) tensor([68, 69,  1])\n",
            "tensor([32, 58,  1]) tensor([70, 71, 69,  1])\n",
            "tensor([32, 59,  1]) tensor([37, 72,  1])\n",
            "tensor([ 9, 35,  1]) tensor([9, 1])\n",
            "tensor([ 9, 35,  1]) tensor([9, 1])\n",
            "tensor([60, 61,  1]) tensor([73, 74,  1])\n",
            "tensor([62, 31,  1]) tensor([15,  1])\n",
            "tensor([62, 31,  1]) tensor([16, 14, 75,  1])\n",
            "tensor([62, 31,  1]) tensor([76,  1])\n",
            "tensor([62, 31,  1]) tensor([77, 78, 14,  1])\n",
            "tensor([16, 63,  1]) tensor([79,  1])\n",
            "tensor([64, 65,  1]) tensor([80,  1])\n",
            "tensor([66, 51,  1]) tensor([81, 82, 83,  1])\n",
            "tensor([67, 35,  1]) tensor([84,  1])\n",
            "tensor([67, 35,  1]) tensor([85,  1])\n",
            "tensor([35, 68,  1]) tensor([86,  1])\n",
            "tensor([69, 29,  1]) tensor([87,  1])\n",
            "tensor([70,  1]) tensor([88,  1])\n",
            "tensor([71, 35,  1]) tensor([89,  1])\n",
            "tensor([72, 29,  1]) tensor([90,  1])\n",
            "tensor([73, 31,  1]) tensor([91,  1])\n",
            "tensor([74, 31,  1]) tensor([92,  1])\n",
            "tensor([75, 35,  1]) tensor([93,  1])\n",
            "tensor([76, 35,  1]) tensor([94, 95,  1])\n",
            "tensor([61, 19,  1]) tensor([96, 21,  1])\n",
            "tensor([77, 29,  1]) tensor([35,  1])\n",
            "tensor([78, 29,  1]) tensor([97, 59,  1])\n",
            "tensor([43, 36,  1]) tensor([51, 98,  1])\n",
            "tensor([43, 37,  1]) tensor([53, 99,  1])\n",
            "tensor([79,  1]) tensor([100,   1])\n",
            "tensor([79,  1]) tensor([101,   1])\n",
            "tensor([ 5, 19,  1]) tensor([102,  21,   1])\n",
            "tensor([44, 80,  1]) tensor([54, 48,  1])\n",
            "tensor([44, 80,  1]) tensor([ 54, 103,   1])\n",
            "tensor([81,  4,  1]) tensor([104, 105,   1])\n",
            "tensor([81,  4,  1]) tensor([106, 105,   1])\n",
            "tensor([81,  4,  1]) tensor([107, 105,   1])\n",
            "tensor([81,  4,  1]) tensor([108, 105,   1])\n",
            "tensor([82, 29,  1]) tensor([109,   1])\n",
            "tensor([83, 84,  1]) tensor([24,  1])\n",
            "tensor([85, 31,  1]) tensor([110,   1])\n",
            "tensor([28, 86,  1]) tensor([111,   1])\n",
            "tensor([28, 37,  1]) tensor([112,   1])\n",
            "tensor([ 2, 87,  1]) tensor([113,   1])\n",
            "tensor([ 2, 88,  1]) tensor([114,   2,   1])\n",
            "tensor([89, 90,  1]) tensor([115,   1])\n",
            "tensor([91, 61,  1]) tensor([ 73, 116,   1])\n",
            "tensor([91, 92,  1]) tensor([117, 116,   1])\n",
            "tensor([32, 93,  1]) tensor([ 68, 118,   1])\n",
            "tensor([94, 95,  1]) tensor([119,   1])\n",
            "tensor([94, 95,  1]) tensor([120, 121,   1])\n",
            "tensor([94, 96,  1]) tensor([122, 123,   1])\n",
            "tensor([94, 97,  1]) tensor([124, 125,   1])\n",
            "tensor([94, 97,  1]) tensor([126, 127, 128,   1])\n",
            "tensor([94, 97,  1]) tensor([129, 130,   1])\n",
            "tensor([98, 29,  1]) tensor([131,   1])\n",
            "tensor([16, 99,  1]) tensor([132,   1])\n",
            "tensor([ 16, 100,   1]) tensor([ 41, 133,   1])\n",
            "tensor([ 16, 101,   1]) tensor([134, 135,   1])\n",
            "tensor([102, 103,   1]) tensor([ 20, 136,   1])\n",
            "tensor([102, 103,   1]) tensor([137, 138,   1])\n",
            "tensor([ 64, 104,   1]) tensor([ 43, 139,   1])\n",
            "tensor([64, 37,  1]) tensor([ 41, 140, 141,   1])\n",
            "tensor([ 64, 105,   1]) tensor([ 41,  28, 142,   1])\n",
            "tensor([ 31, 106,   1]) tensor([143,   1])\n",
            "tensor([ 31, 107,   1]) tensor([144,   1])\n",
            "tensor([ 31, 107,   1]) tensor([145,   1])\n",
            "tensor([ 31, 107,   1]) tensor([146,   1])\n",
            "tensor([108,   2,   1]) tensor([147,   1])\n",
            "tensor([109,  55,   1]) tensor([148,   1])\n",
            "tensor([110, 111,   1]) tensor([149, 150,   1])\n",
            "tensor([112,  86,   1]) tensor([151,   1])\n",
            "tensor([112, 113,   1]) tensor([152, 151,   1])\n",
            "tensor([114,  29,   1]) tensor([153,  93,   1])\n",
            "tensor([115,  29,   1]) tensor([154,   1])\n",
            "tensor([75, 61,  1]) tensor([155,  93,   1])\n",
            "tensor([75, 61,  1]) tensor([156,  93,   1])\n",
            "tensor([116,  19,   1]) tensor([157,  21,   1])\n",
            "tensor([116,  19,   1]) tensor([ 70, 158,  21,   1])\n",
            "tensor([ 61, 111,   1]) tensor([ 96, 159,   1])\n",
            "tensor([ 61, 117,   1]) tensor([ 96, 160,   1])\n",
            "tensor([ 61, 117,   1]) tensor([161, 160,   1])\n",
            "tensor([ 61, 118,   1]) tensor([ 96, 162, 163,   1])\n",
            "tensor([ 61, 118,   1]) tensor([ 96, 164,   1])\n",
            "tensor([61, 37,  1]) tensor([ 96, 165,   1])\n",
            "tensor([ 61, 119,   1]) tensor([ 96, 166,   1])\n",
            "tensor([61, 38,  1]) tensor([ 96, 167,   1])\n",
            "tensor([ 61, 120,   1]) tensor([ 96, 168, 169,   1])\n",
            "tensor([ 68, 104,   1]) tensor([129, 170,   1])\n",
            "tensor([121,  35,   1]) tensor([171, 172,   1])\n",
            "tensor([ 18, 122,   1]) tensor([173,  59,   1])\n",
            "tensor([ 18, 123,   1]) tensor([174, 175,   1])\n",
            "tensor([ 18, 124,   1]) tensor([176, 177,   1])\n",
            "tensor([43, 63,  1]) tensor([51, 79,  1])\n",
            "tensor([125, 126,   1]) tensor([178, 179,   1])\n",
            "tensor([125, 127,   1]) tensor([180,   1])\n",
            "tensor([  5, 128,  16,   1]) tensor([ 41, 181,   1])\n",
            "tensor([  5, 111,   1]) tensor([102, 159,   1])\n",
            "tensor([ 5, 38,  1]) tensor([102, 167,   1])\n",
            "tensor([129,  81,   1]) tensor([182, 183,   1])\n",
            "tensor([129,  81,   1]) tensor([184, 185,   1])\n",
            "tensor([130,  35,   1]) tensor([186,   1])\n",
            "tensor([131,   1]) tensor([187,   1])\n",
            "tensor([132, 133,   1]) tensor([188, 189, 190,   1])\n",
            "tensor([47, 86,  1]) tensor([24,  1])\n",
            "tensor([ 52, 113,   1]) tensor([191, 192,   1])\n",
            "tensor([ 52, 134,   1]) tensor([193, 192,   1])\n",
            "tensor([135, 136,   1]) tensor([194, 195,   1])\n",
            "tensor([137, 138,   1]) tensor([196, 197, 198,   1])\n",
            "tensor([137, 138,   1]) tensor([196, 197, 199,   1])\n",
            "tensor([139,   1]) tensor([200,   1])\n",
            "tensor([140, 141,   1]) tensor([201, 202,   1])\n",
            "tensor([142,  35,   1]) tensor([203,   1])\n",
            "tensor([142,  51,   1]) tensor([204,   1])\n",
            "tensor([142,  51,   1]) tensor([205,   1])\n",
            "tensor([143,  31,   1]) tensor([206,   1])\n",
            "tensor([143,  35,   1]) tensor([171, 207,   1])\n",
            "tensor([143,  35,   1]) tensor([171, 206,   1])\n",
            "tensor([ 28, 144,   1]) tensor([208,   1])\n",
            "tensor([ 89, 145,   1]) tensor([209, 210,   1])\n",
            "tensor([ 89, 145,   1]) tensor([ 27, 211, 212,   1])\n",
            "tensor([ 91, 124,   1]) tensor([213, 214,   1])\n",
            "tensor([146,  84,   1]) tensor([215, 198,   1])\n",
            "tensor([ 62, 124,   1]) tensor([213,  78,  14,   1])\n",
            "tensor([ 94, 147,   1]) tensor([216,   1])\n",
            "tensor([ 94, 147,   1]) tensor([120, 217, 218,   1])\n",
            "tensor([ 16, 148,   1]) tensor([ 41, 219,   1])\n",
            "tensor([ 16, 149,   1]) tensor([ 41, 220,   1])\n",
            "tensor([ 16, 150,   1]) tensor([221,   1])\n",
            "tensor([ 64, 151, 152,   1]) tensor([ 41, 222,   1])\n",
            "tensor([64, 25,  1]) tensor([223,   1])\n",
            "tensor([64, 25,  1]) tensor([224,   1])\n",
            "tensor([64, 25,  1]) tensor([225,   1])\n",
            "tensor([64, 25,  1]) tensor([226,   1])\n",
            "tensor([153,  37,   1]) tensor([20, 99,  1])\n",
            "tensor([ 31, 154,   1]) tensor([227,  69,   1])\n",
            "tensor([ 31, 154,   1]) tensor([227, 228,   1])\n",
            "tensor([ 31, 155,   1]) tensor([229,  69,   1])\n",
            "tensor([ 31, 156,   1]) tensor([230,   1])\n",
            "tensor([157,  89,   1]) tensor([231,   1])\n",
            "tensor([157,  89,   1]) tensor([232,   1])\n",
            "tensor([158,  56,   1]) tensor([233,   1])\n",
            "tensor([159, 141,   1]) tensor([201, 234,   1])\n",
            "tensor([160, 161,   1]) tensor([235, 236,   1])\n",
            "tensor([109, 162,   1]) tensor([237,   1])\n",
            "tensor([109, 162,   1]) tensor([238, 239,   1])\n",
            "tensor([ 97, 163,   1]) tensor([240, 241,   1])\n",
            "tensor([164, 165,   1]) tensor([242,   1])\n",
            "tensor([164, 165,   1]) tensor([243,   1])\n",
            "tensor([166, 124,   1]) tensor([244, 245,   1])\n",
            "tensor([167,  15,   1]) tensor([246,   1])\n",
            "tensor([ 17, 168,   1]) tensor([247, 248,   1])\n",
            "tensor([ 17, 168,   1]) tensor([247,  75,   1])\n",
            "tensor([ 17, 169,   1]) tensor([249,  75,   1])\n",
            "tensor([170,   1]) tensor([250,   1])\n",
            "tensor([110, 171,   1]) tensor([149, 251,   1])\n",
            "tensor([110, 172,   1]) tensor([149, 252,   1])\n",
            "tensor([112,  88,   1]) tensor([253, 151,   1])\n",
            "tensor([112, 173,   1]) tensor([254,  14,   1])\n",
            "tensor([174, 161,   1]) tensor([235,  17,   1])\n",
            "tensor([ 12, 162,   1]) tensor([233,  14,   1])\n",
            "tensor([12, 47,  1]) tensor([57,  1])\n",
            "tensor([ 12, 113,   1]) tensor([255,  14,   1])\n",
            "tensor([175, 162,   1]) tensor([233,   1])\n",
            "tensor([ 74, 113,   1]) tensor([256, 257,   1])\n",
            "tensor([176, 177,   1]) tensor([258,   1])\n",
            "tensor([176, 124,   1]) tensor([259,   1])\n",
            "tensor([176, 124,   1]) tensor([176, 259,   1])\n",
            "tensor([176, 124,   1]) tensor([244, 259,   1])\n",
            "tensor([178,  81,   1]) tensor([50,  1])\n",
            "tensor([179, 125,   1]) tensor([260,   1])\n",
            "tensor([116, 117,   1]) tensor([157, 160,   1])\n",
            "tensor([116, 117,   1]) tensor([ 70, 261, 160,   1])\n",
            "tensor([116, 117,   1]) tensor([262, 160,   1])\n",
            "tensor([116, 118,   1]) tensor([157, 196, 263,   1])\n",
            "tensor([116,  37,   1]) tensor([157, 165,   1])\n",
            "tensor([116,  37,   1]) tensor([ 70, 158, 165,   1])\n",
            "tensor([ 61, 171,   1]) tensor([161, 264,   1])\n",
            "tensor([ 61, 180,   1]) tensor([ 96, 265, 266,   1])\n",
            "tensor([ 61, 181,   1]) tensor([ 96, 267,   1])\n",
            "tensor([ 61, 182,   1]) tensor([161, 268,  14,   1])\n",
            "tensor([ 61, 183,   1]) tensor([161, 269,   1])\n",
            "tensor([ 61, 183,   1]) tensor([161, 270,   1])\n",
            "tensor([ 18, 184,   1]) tensor([271, 272, 273, 274,   1])\n",
            "tensor([185, 117,   1]) tensor([275, 276,   1])\n",
            "tensor([185, 117,   1]) tensor([277,   1])\n",
            "tensor([ 13, 113,   1]) tensor([256,  15,   1])\n",
            "tensor([186,  55,   1]) tensor([148,   1])\n",
            "tensor([ 43, 187,  31,   1]) tensor([ 51,  77, 278,   1])\n",
            "tensor([ 43, 188,   1]) tensor([279, 280, 281,   1])\n",
            "tensor([ 43, 189,   1]) tensor([279, 282,   1])\n",
            "tensor([190, 191,   1]) tensor([ 27, 163,   1])\n",
            "tensor([  5, 192,   1]) tensor([102, 283,   1])\n",
            "tensor([  5, 182,   1]) tensor([102,  40,   1])\n",
            "tensor([193,   1]) tensor([284,   1])\n",
            "tensor([ 81, 194,   1]) tensor([285, 286,   1])\n",
            "tensor([ 81, 172,   1]) tensor([287,   1])\n",
            "tensor([195, 196,   1]) tensor([288, 289,   1])\n",
            "tensor([197, 198,   1]) tensor([290, 291,   1])\n",
            "tensor([ 46, 199,   1]) tensor([148,   1])\n",
            "tensor([ 46, 200,   1]) tensor([292,   1])\n",
            "tensor([ 46, 200,   1]) tensor([293, 259,   1])\n",
            "tensor([132, 201,   1]) tensor([188, 294,   1])\n",
            "tensor([202,  53,   1]) tensor([295, 296,   1])\n",
            "tensor([203, 204,   1]) tensor([297, 298,   1])\n",
            "tensor([203, 204,   1]) tensor([299, 298,   1])\n",
            "tensor([205, 124,   1]) tensor([176, 300,   1])\n",
            "tensor([205, 124,   1]) tensor([176, 301,   1])\n",
            "tensor([206, 124,   1]) tensor([176, 302,   1])\n",
            "tensor([207, 208,   1]) tensor([303, 304,   1])\n",
            "tensor([ 52, 184,   1]) tensor([271, 192,   1])\n",
            "tensor([ 52, 209,   1]) tensor([305, 192,   1])\n",
            "tensor([ 52, 210,   1]) tensor([203,   1])\n",
            "tensor([ 52, 211,   1]) tensor([306, 192,   1])\n",
            "tensor([212,   1]) tensor([307,   1])\n",
            "tensor([137, 213,   1]) tensor([308, 198,   1])\n",
            "tensor([214, 215,   1]) tensor([309, 310,   1])\n",
            "tensor([216, 217,   1]) tensor([311, 312,   1])\n",
            "tensor([  8, 218,   1]) tensor([313,   1])\n",
            "tensor([142,  92,   1]) tensor([314, 315, 316,   1])\n",
            "tensor([143,  61,   1]) tensor([161, 206,   1])\n",
            "tensor([219,  35,   1]) tensor([171, 317,   1])\n",
            "tensor([219,  51,   1]) tensor([318, 317,  95,   1])\n",
            "tensor([219,  51,   1]) tensor([319, 317,  95,   1])\n",
            "tensor([220, 221,   1]) tensor([320, 321,   1])\n",
            "tensor([ 32, 222,  29,   1]) tensor([ 37, 322, 323,   1])\n",
            "tensor([ 32, 223,  97,   1]) tensor([324, 325,   1])\n",
            "tensor([ 32, 223, 224,   1]) tensor([ 70, 326, 327, 328,   1])\n",
            "tensor([ 32, 149,   1]) tensor([ 37, 329,   1])\n",
            "tensor([225,  22,   1]) tensor([330,  24,   1])\n",
            "tensor([ 62, 226,   1]) tensor([331, 332,   1])\n",
            "tensor([ 62, 226,   1]) tensor([331,  14,   1])\n",
            "tensor([ 94, 227,   1]) tensor([333, 334,   1])\n",
            "tensor([ 94, 228,   1]) tensor([129,  80,   1])\n",
            "tensor([ 94, 228,   1]) tensor([335, 336,   1])\n",
            "tensor([229,  39,   1]) tensor([337,  27, 338,   1])\n",
            "tensor([229,  39,   1]) tensor([337, 339,   1])\n",
            "tensor([ 98, 162,   1]) tensor([306, 192,   1])\n",
            "tensor([ 98, 134,   1]) tensor([306, 193, 147,   1])\n",
            "tensor([ 16, 128, 230,   1]) tensor([ 41, 340,   1])\n",
            "tensor([ 16, 231,   1]) tensor([134, 341, 220,   1])\n",
            "tensor([ 16, 232,   1]) tensor([134, 342,   1])\n",
            "tensor([ 16, 233,  92,   1]) tensor([ 70,  71, 343,   1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.embeding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size)\n",
        "  def forward(self, x, h):\n",
        "    # 배치, 시계열 차원\n",
        "    x = self.embeding(x).view(1,1,-1)\n",
        "    output,hidden = self.gru(x,h)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "CfTrvznpDQw2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "   def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=11):\n",
        "       super(Decoder, self).__init__()\n",
        "\n",
        "       # 임베딩층 정의\n",
        "       self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "       # 어텐션 가중치를 계산하기 위한 MLP층\n",
        "       self.attention = nn.Linear(hidden_size * 2, max_length)\n",
        "\n",
        "       #특징 추출을 위한 MLP층\n",
        "       self.context = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "       # 과적합을 피하기 위한 드롭아웃 층\n",
        "       self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "       # GRU층\n",
        "       self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "       # 단어 분류를 위한 MLP층\n",
        "       self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "       # 활성화 함수\n",
        "       self.relu = nn.ReLU()\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "   def forward(self, x, h, encoder_outputs):\n",
        "       # 입력을 밀집 표현으로\n",
        "       x = self.embedding(x).view(1, 1, -1)\n",
        "       x = self.dropout(x)\n",
        "\n",
        "       # 어텐션 가중치 계산\n",
        "       attn_weights = self.softmax(\n",
        "           self.attention(torch.cat((x[0], h[0]), -1)))\n",
        "\n",
        "       # 어텐션 가중치와 인코더의 출력을 내적\n",
        "       attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                encoder_outputs.unsqueeze(0))\n",
        "\n",
        "       # 인코더 각 시점의 중요도와 민집표현을 합쳐\n",
        "       # MLP층으로 특징 추출\n",
        "       output = torch.cat((x[0], attn_applied[0]), 1)\n",
        "       output = self.context(output).unsqueeze(0)\n",
        "       output = self.relu(output)\n",
        "\n",
        "       # GRU층으로 입력\n",
        "       output, hidden = self.gru(output, h)\n",
        "\n",
        "       # 예측된 단어 출력\n",
        "       output = self.out(output[0])\n",
        "\n",
        "       return output"
      ],
      "metadata": {
        "id": "fvK2KATeFH0W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습"
      ],
      "metadata": {
        "id": "x6RI23V0MoLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tqdm\n",
        "\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "\n",
        "# 학습에 사용할 프로세서 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# 학습에 사용할 데이터셋 정의\n",
        "dataset = Eng2Kor()\n",
        "\n",
        "\n",
        "# 인코더 디코더 정의\n",
        "encoder = Encoder(input_size=len(dataset.engBOW), hidden_size=64).to(device)\n",
        "decoder = Decoder(64, len(dataset.korBOW), dropout_p=0.1).to(device)\n",
        "# 인코더 디코더 학습을 위한 최적화 정의\n",
        "encoder_optimizer = Adam(encoder.parameters(), lr=0.0001)\n",
        "decoder_optimizer = Adam(decoder.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaY44aJqMHJP",
        "outputId": "3b185cc9-fc8a-4e35-a5be-690bec05e357"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list index out of range ['']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "   iterator = tqdm.tqdm(loader(dataset), total=len(dataset))\n",
        "   total_loss = 0\n",
        "\n",
        "   for data, label in iterator:\n",
        "       data = torch.tensor(data, dtype=torch.long).to(device)\n",
        "       label = torch.tensor(label, dtype=torch.long).to(device)\n",
        "\n",
        "       # 인코더의 초기 은닉 상태\n",
        "       encoder_hidden = torch.zeros(1, 1, 64).to(device)\n",
        "       # 인코더의 모든 시점의 출력을 저장하는 변수\n",
        "       encoder_outputs = torch.zeros(11, 64).to(device)\n",
        "\n",
        "       encoder_optimizer.zero_grad()\n",
        "       decoder_optimizer.zero_grad()\n",
        "\n",
        "       loss = 0\n",
        "       for ei in range(len(data)):\n",
        "           # 한 단어씩 인코더에 넣어줌\n",
        "           encoder_output, encoder_hidden = encoder(\n",
        "               data[ei], encoder_hidden)\n",
        "           # 인코더의 은닉 상태를 저장\n",
        "           encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "       decoder_input = torch.tensor([[0]]).to(device)\n",
        "\n",
        "       # 인코더의 마지막 은닉 상태를 디코더의 초기 은닉 상태로 저장\n",
        "       decoder_hidden = encoder_hidden\n",
        "       # 티치포싱..... 디코더가 학습이 안되면 시간이 오래걸려서 강제로\n",
        "       # 정답을 50%의 비율로 넣어준다\n",
        "       use_teacher_forcing = True if random.random() < 0.5 else False\n",
        "\n",
        "       if use_teacher_forcing:\n",
        "           for di in range(len(label)):\n",
        "               decoder_output = decoder(\n",
        "                   decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "               # 직접적으로 정답을 다음 시점의 입력으로 넣어줌\n",
        "               target = torch.tensor(label[di], dtype=torch.long).to(device)\n",
        "               target = torch.unsqueeze(target, dim=0).to(device)\n",
        "               loss += nn.CrossEntropyLoss()(decoder_output, target)\n",
        "               decoder_input = target\n",
        "       else:\n",
        "           for di in range(len(label)):\n",
        "               decoder_output = decoder(\n",
        "                   decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "               # 가장 높은 확률을 갖는 단어의 인덱스가 topi\n",
        "               topv, topi = decoder_output.topk(1)\n",
        "               decoder_input = topi.squeeze().detach()\n",
        "\n",
        "               # 디코더의 예측값을 다음 시점의 입력으로 넣어줌\n",
        "               target = torch.tensor(label[di], dtype=torch.long).to(device)\n",
        "               target = torch.unsqueeze(target, dim=0).to(device)\n",
        "               loss += nn.CrossEntropyLoss()(decoder_output, target)\n",
        "\n",
        "               if decoder_input.item() == 1:  # <EOS> 토큰을 만나면 중지\n",
        "                   break\n",
        "       # 전체 손실 계산\n",
        "       total_loss += loss.item()/len(dataset)\n",
        "       iterator.set_description(f\"epoch:{epoch+1} loss:{total_loss}\")\n",
        "       loss.backward()\n",
        "\n",
        "       encoder_optimizer.step()\n",
        "       decoder_optimizer.step()\n",
        "\n",
        "torch.save(encoder.state_dict(), \"attn_enc.pth\")\n",
        "torch.save(decoder.state_dict(), \"attn_dec.pth\")"
      ],
      "metadata": {
        "id": "g4daM26xMsnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2976a49-eba9-4098-93e1-a5f34975a1d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/304 [00:00<?, ?it/s]<ipython-input-15-dcdc4ec7f0e5>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data, dtype=torch.long).to(device)\n",
            "<ipython-input-15-dcdc4ec7f0e5>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label, dtype=torch.long).to(device)\n",
            "<ipython-input-15-dcdc4ec7f0e5>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(label[di], dtype=torch.long).to(device)\n",
            "epoch:1 loss:0.04131637748919035:   0%|          | 1/304 [00:01<06:42,  1.33s/it]<ipython-input-15-dcdc4ec7f0e5>:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(label[di], dtype=torch.long).to(device)\n",
            "epoch:1 loss:12.122290059139853: 100%|██████████| 304/304 [00:04<00:00, 68.41it/s]\n",
            "epoch:2 loss:8.858165384907466: 100%|██████████| 304/304 [00:02<00:00, 103.89it/s]\n",
            "epoch:3 loss:8.779785455841761: 100%|██████████| 304/304 [00:03<00:00, 86.10it/s]\n",
            "epoch:4 loss:8.21575344393128: 100%|██████████| 304/304 [00:02<00:00, 102.78it/s]\n",
            "epoch:5 loss:8.128517308517509: 100%|██████████| 304/304 [00:02<00:00, 106.18it/s]\n",
            "epoch:6 loss:8.06976097979044: 100%|██████████| 304/304 [00:02<00:00, 104.02it/s]\n",
            "epoch:7 loss:8.602871857975659: 100%|██████████| 304/304 [00:03<00:00, 84.61it/s]\n",
            "epoch:8 loss:8.558804310466117: 100%|██████████| 304/304 [00:03<00:00, 100.94it/s]\n",
            "epoch:9 loss:8.651702953012363: 100%|██████████| 304/304 [00:03<00:00, 100.43it/s]\n",
            "epoch:10 loss:8.646623185590697: 100%|██████████| 304/304 [00:03<00:00, 87.18it/s]\n",
            "epoch:11 loss:8.757419561084943: 100%|██████████| 304/304 [00:04<00:00, 66.13it/s]\n",
            "epoch:12 loss:8.62136967872318: 100%|██████████| 304/304 [00:03<00:00, 96.65it/s]\n",
            "epoch:13 loss:8.247034799895784: 100%|██████████| 304/304 [00:03<00:00, 97.77it/s]\n",
            "epoch:14 loss:8.133163995648685: 100%|██████████| 304/304 [00:03<00:00, 96.95it/s]\n",
            "epoch:15 loss:8.026323880019943: 100%|██████████| 304/304 [00:03<00:00, 82.01it/s]\n",
            "epoch:16 loss:7.742663947375199: 100%|██████████| 304/304 [00:03<00:00, 97.23it/s]\n",
            "epoch:17 loss:7.734394149560671: 100%|██████████| 304/304 [00:03<00:00, 96.65it/s]\n",
            "epoch:18 loss:7.470664421978754: 100%|██████████| 304/304 [00:03<00:00, 95.29it/s]\n",
            "epoch:19 loss:7.452609595891675: 100%|██████████| 304/304 [00:03<00:00, 81.73it/s]\n",
            "epoch:20 loss:7.153133454683584: 100%|██████████| 304/304 [00:03<00:00, 93.32it/s]\n",
            "epoch:21 loss:7.0247497899751865: 100%|██████████| 304/304 [00:03<00:00, 94.66it/s]\n",
            "epoch:22 loss:6.91123028766168: 100%|██████████| 304/304 [00:03<00:00, 85.19it/s]\n",
            "epoch:23 loss:6.849908943239016: 100%|██████████| 304/304 [00:03<00:00, 87.19it/s]\n",
            "epoch:24 loss:6.57962607082568: 100%|██████████| 304/304 [00:03<00:00, 95.16it/s]\n",
            "epoch:25 loss:6.510107661156279: 100%|██████████| 304/304 [00:03<00:00, 95.64it/s]\n",
            "epoch:26 loss:6.299919621332696: 100%|██████████| 304/304 [00:03<00:00, 81.81it/s]\n",
            "epoch:27 loss:6.145535355531855: 100%|██████████| 304/304 [00:03<00:00, 94.50it/s]\n",
            "epoch:28 loss:6.009218159866959: 100%|██████████| 304/304 [00:03<00:00, 93.95it/s]\n",
            "epoch:29 loss:5.932669608608674: 100%|██████████| 304/304 [00:03<00:00, 94.18it/s]\n",
            "epoch:30 loss:5.851912760420852: 100%|██████████| 304/304 [00:03<00:00, 80.70it/s]\n",
            "epoch:31 loss:5.683113980842262: 100%|██████████| 304/304 [00:03<00:00, 94.89it/s]\n",
            "epoch:32 loss:5.738898105919361: 100%|██████████| 304/304 [00:03<00:00, 93.37it/s]\n",
            "epoch:33 loss:5.347516019681567: 100%|██████████| 304/304 [00:03<00:00, 87.72it/s]\n",
            "epoch:34 loss:5.340425647403063: 100%|██████████| 304/304 [00:03<00:00, 84.19it/s]\n",
            "epoch:35 loss:5.066232316784169: 100%|██████████| 304/304 [00:03<00:00, 94.42it/s]\n",
            "epoch:36 loss:5.138951668417766: 100%|██████████| 304/304 [00:03<00:00, 94.72it/s]\n",
            "epoch:37 loss:4.98479262298267: 100%|██████████| 304/304 [00:03<00:00, 79.67it/s]\n",
            "epoch:38 loss:4.864084074567807: 100%|██████████| 304/304 [00:03<00:00, 90.15it/s]\n",
            "epoch:39 loss:4.759393749758599: 100%|██████████| 304/304 [00:03<00:00, 93.33it/s]\n",
            "epoch:40 loss:4.583362150741252: 100%|██████████| 304/304 [00:03<00:00, 92.57it/s]\n",
            "epoch:41 loss:4.437264871342405: 100%|██████████| 304/304 [00:03<00:00, 79.84it/s]\n",
            "epoch:42 loss:4.468077720289956: 100%|██████████| 304/304 [00:03<00:00, 90.18it/s]\n",
            "epoch:43 loss:4.3718180244690465: 100%|██████████| 304/304 [00:03<00:00, 92.21it/s]\n",
            "epoch:44 loss:4.2433021688147585: 100%|██████████| 304/304 [00:03<00:00, 85.34it/s]\n",
            "epoch:45 loss:4.163626854455: 100%|██████████| 304/304 [00:03<00:00, 84.47it/s]\n",
            "epoch:46 loss:3.9921560655102937: 100%|██████████| 304/304 [00:03<00:00, 91.81it/s]\n",
            "epoch:47 loss:3.9230592034168916: 100%|██████████| 304/304 [00:03<00:00, 89.34it/s]\n",
            "epoch:48 loss:3.9384607881503664: 100%|██████████| 304/304 [00:03<00:00, 76.93it/s]\n",
            "epoch:49 loss:3.8210678873583688: 100%|██████████| 304/304 [00:03<00:00, 91.52it/s]\n",
            "epoch:50 loss:3.6575558596829842: 100%|██████████| 304/304 [00:03<00:00, 92.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "성능평가"
      ],
      "metadata": {
        "id": "5rk5O63jS4t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 가중치 불러오기\n",
        "encoder.load_state_dict(torch.load(\"attn_enc.pth\", map_location=device))\n",
        "# 디코더 가중치 불러오기\n",
        "decoder.load_state_dict(torch.load(\"attn_dec.pth\", map_location=device))\n",
        "\n",
        "\n",
        "# 불러올 영어 문장을 랜덤하게 지정\n",
        "idx = random.randint(0, len(dataset))\n",
        "# 테스트에 사용할 문장\n",
        "input_sentence = dataset.eng_corpus[idx]\n",
        "# 신경망이 번역한 문장\n",
        "pred_sentence = \"\"\n",
        "\n",
        "data, label = dataset[idx]\n",
        "data = torch.tensor(data, dtype=torch.long).to(device)\n",
        "label = torch.tensor(label, dtype=torch.long).to(device)\n",
        "\n",
        "# 인코더의 초기 은닉 상태 정의\n",
        "encoder_hidden = torch.zeros(1, 1, 64).to(device)\n",
        "# 인코더 출력을 담기위한 변수\n",
        "encoder_outputs = torch.zeros(11, 64).to(device)"
      ],
      "metadata": {
        "id": "s7JzdAeRQMx3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더"
      ],
      "metadata": {
        "id": "Peubu9QeVQmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ei in range(len(data)):\n",
        "   # 한 단어씩 인코더에 넣어줌\n",
        "   encoder_output, encoder_hidden = encoder(\n",
        "       data[ei], encoder_hidden)\n",
        "\n",
        "   # 인코더의 출력을 저장\n",
        "   encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "\n",
        "# 디코더의 초기 입력\n",
        "# 0은 <SOS>토큰\n",
        "decoder_input = torch.tensor([[0]]).to(device)\n",
        "\n",
        "# 인코더의 마지막 은닉 상태를 디코더의 초기 은닉 상태로\n",
        "decoder_hidden = encoder_hidden"
      ],
      "metadata": {
        "id": "hsrJZtjWTSp5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "디코더"
      ],
      "metadata": {
        "id": "-mUe7VdBVVq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for di in range(11):\n",
        "    # 가장 높은 확률을 갖는 단어의 요소를 구함\n",
        "   decoder_output = decoder(\n",
        "                       decoder_input, decoder_hidden, encoder_outputs)\n",
        "   topv, topi = decoder_output.topk(1)\n",
        "   decoder_input = topi.squeeze().detach()\n",
        "\n",
        "   # <EOS> 토큰을 만나면 중지\n",
        "   if decoder_input.item() == 1:\n",
        "       break\n",
        "\n",
        "   # 가장 높은 단어를 문자열에 추가\n",
        "   pred_sentence += list(dataset.korBOW.keys())[decoder_input] + \" \"\n",
        "\n",
        "print(input_sentence)  # 영어 문장\n",
        "print(pred_sentence)  # 한글 문장"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we862LpCVUUn",
        "outputId": "19b2ac1e-d31d-4077-90b4-baebf3c18ef8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "come here\n",
            "여기로 와 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKqUP81TVXdO"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}